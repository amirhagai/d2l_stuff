{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "intuition for object detection in an image\n",
    "by enumerating a few desiderata to guide our design\n",
    "of a neural network architecture suitable for computer vision:\n",
    "\n",
    "1. In the earliest layers, our network\n",
    "   should respond similarly to the same patch,\n",
    "   regardless of where it appears in the image. This principle is called *translation invariance* (or *translation equivariance*).\n",
    "1. The earliest layers of the network should focus on local regions,\n",
    "   without regard for the contents of the image in distant regions. This is the *locality* principle.\n",
    "   Eventually, these local representations can be aggregated\n",
    "   to make predictions at the whole image level.\n",
    "1. As we proceed, deeper layers should be able to capture longer-range features of the \n",
    "   image, in a way similar to higher level vision in nature. \n",
    "\n",
    "Let's see how this translates into mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, k):\n",
    "\n",
    "    h, w = k.shape\n",
    "    y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "\n",
    "    for i in range(y.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            y[i, j] = (k * X[i : i + h, j : j + w]).sum()\n",
    "\n",
    "    return y\n",
    "\n",
    "def test_corr2d():\n",
    "    con = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, bias=False)\n",
    "    weight = con.weight.data[0, 0].clone().detach()\n",
    "    x = torch.rand((6, 6))\n",
    "    x.requires_grad = True\n",
    "    res1 = corr2d(x, weight)\n",
    "    res1.backward(torch.ones_like(res1))\n",
    "\n",
    "    x2 = x.clone().detach().requires_grad_(True)\n",
    "    res2 = con(x2[None, None, :, :])\n",
    "    res2.backward(torch.ones_like(res2))\n",
    "\n",
    "\n",
    "    assert (corr2d(x, weight) - con(x[None, None, :, :])[0, 0] < 1e-5).sum() == 4 * 4, f\"problem occour, corr2d result is {corr2d(x, weight)} and conv {con(x[None, None, :, :])[0, 0]}\"\n",
    "    assert (x.grad - x2.grad < 1e-5).sum() == 6 * 6, f\"problem occour, x grad is {x.grad} and x2 grad {x2.grad}\"\n",
    "\n",
    "test_corr2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class Conv2D(nn.Module):\n",
    "\n",
    "    def __init__(self, kernel_size) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.rand(1))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return corr2d(X, self.weight) + self.bias\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.]]),\n",
       " tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.]]),\n",
       " tensor([[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#edge detection - vertical\n",
    "\n",
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "\n",
    "K = torch.tensor([[1.0, -1.0]])\n",
    "\n",
    "Y = corr2d(X, K)\n",
    "X, Y, corr2d(X.t(), K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 500, loss 0.955, min loss 0.09738\n",
      "epoch 1000, loss 9.858, min loss 0.00369\n",
      "epoch 1500, loss 3.367, min loss 0.00369\n",
      "\n",
      "best fit - tensor([[[[ 1.0025, -1.0159]]]])\n",
      "last results - tensor([[[[ 0.9246, -1.3891]]]])\n"
     ]
    }
   ],
   "source": [
    "# learinng the edge detector!!\n",
    "\n",
    "# we will use X as our input, Y as our wanted output\n",
    "\n",
    "conv = nn.LazyConv2d(out_channels=1, kernel_size=(1, 2), bias=False)\n",
    "lr = 1e-4\n",
    "\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "\n",
    "min_loss = 10\n",
    "best = None\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1501):\n",
    "\n",
    "    res = conv(X)\n",
    "    loss = (res - Y) ** 2\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if loss.sum().item() < min_loss:\n",
    "\n",
    "            min_loss = loss.sum().item()\n",
    "            best = conv.weight.data.clone()\n",
    "    loss.sum().backward()\n",
    "    # the use of [:] makes the operation in place\n",
    "    conv.weight.data[:] += -lr * conv.weight.grad\n",
    "\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f'epoch {epoch + 1}, loss {loss.sum():.3f}, min loss {min_loss:.5f}')\n",
    "\n",
    "print()\n",
    "print(f\"best fit - {best}\")\n",
    "print(f\"last results - {conv.weight.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "As described above, one tricky issue when applying convolutional layers\n",
    "is that we tend to lose pixels on the perimeter of our image. Consider :numref:`img_conv_reuse` that depicts the pixel utilization as a function of the convolution kernel size and the position within the image. The pixels in the corners are hardly used at all. \n",
    "\n",
    "![Pixel utilization for convolutions of size $1 \\times 1$, $2 \\times 2$, and $3 \\times 3$ respectively.](../img/conv-reuse.svg)\n",
    ":label:`img_conv_reuse`\n",
    "\n",
    "Since we typically use small kernels,\n",
    "for any given convolution\n",
    "we might only lose a few pixels\n",
    "but this can add up as we apply\n",
    "many successive convolutional layers.\n",
    "One straightforward solution to this problem\n",
    "is to add extra pixels of filler around the boundary of our input image,\n",
    "thus increasing the effective size of the image.\n",
    "Typically, we set the values of the extra pixels to zero.\n",
    "In :numref:`img_conv_pad`, we pad a $3 \\times 3$ input,\n",
    "increasing its size to $5 \\times 5$.\n",
    "The corresponding output then increases to a $4 \\times 4$ matrix.\n",
    "The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $0\\times0+0\\times1+0\\times2+0\\times3=0$.\n",
    "\n",
    "![Two-dimensional cross-correlation with padding.](../img/conv-pad.svg)\n",
    ":label:`img_conv_pad`\n",
    "\n",
    "In general, if we add a total of $p_\\textrm{h}$ rows of padding\n",
    "(roughly half on top and half on bottom)\n",
    "and a total of $p_\\textrm{w}$ columns of padding\n",
    "(roughly half on the left and half on the right),\n",
    "the output shape will be\n",
    "\n",
    "$$(n_\\textrm{h}-k_\\textrm{h}+p_\\textrm{h}+1)\\times(n_\\textrm{w}-k_\\textrm{w}+p_\\textrm{w}+1).$$\n",
    "\n",
    "This means that the height and width of the output\n",
    "will increase by $p_\\textrm{h}$ and $p_\\textrm{w}$, respectively.\n",
    "\n",
    "In many cases, we will want to set $p_\\textrm{h}=k_\\textrm{h}-1$ and $p_\\textrm{w}=k_\\textrm{w}-1$\n",
    "to give the input and output the same height and width.\n",
    "This will make it easier to predict the output shape of each layer\n",
    "when constructing the network.\n",
    "Assuming that $k_\\textrm{h}$ is odd here,\n",
    "we will pad $p_\\textrm{h}/2$ rows on both sides of the height.\n",
    "If $k_\\textrm{h}$ is even, one possibility is to\n",
    "pad $\\lceil p_\\textrm{h}/2\\rceil$ rows on the top of the input\n",
    "and $\\lfloor p_\\textrm{h}/2\\rfloor$ rows on the bottom.\n",
    "We will pad both sides of the width in the same way.\n",
    "\n",
    "CNNs commonly use convolution kernels\n",
    "with odd height and width values, such as 1, 3, 5, or 7.\n",
    "Choosing odd kernel sizes has the benefit\n",
    "that we can preserve the dimensionality\n",
    "while padding with the same number of rows on top and bottom,\n",
    "and the same number of columns on left and right.\n",
    "\n",
    "Moreover, this practice of using odd kernels\n",
    "and padding to precisely preserve dimensionality\n",
    "offers a clerical benefit.\n",
    "For any two-dimensional tensor `X`,\n",
    "when the kernel's size is odd\n",
    "and the number of padding rows and columns\n",
    "on all sides are the same,\n",
    "thereby producing an output with the same height and width as the input,\n",
    "we know that the output `Y[i, j]` is calculated\n",
    "by cross-correlation of the input and convolution kernel\n",
    "with the window centered on `X[i, j]`.\n",
    "\n",
    "In the following example, we create a two-dimensional convolutional layer\n",
    "with a height and width of 3\n",
    "and (**apply 1 pixel of padding on all sides.**)\n",
    "Given an input with a height and width of 8,\n",
    "we find that the height and width of the output is also 8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\e046357\\Downloads\\WPy64-31160\\python-3.11.6.amd64\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We define a helper function to calculate convolutions. It initializes the\n",
    "# convolutional layer weights and performs corresponding dimensionality\n",
    "# elevations and reductions on the input and output\n",
    "def comp_conv2d(conv2d, X):\n",
    "    # (1, 1) indicates that batch size and the number of channels are both 1\n",
    "    X = X.reshape((1, 1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    # Strip the first two dimensions: examples and channels\n",
    "    return Y.reshape(Y.shape[2:])\n",
    "\n",
    "# 1 row and column is padded on either side, so a total of 2 rows or columns\n",
    "# are added\n",
    "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1)\n",
    "X = torch.rand(size=(8, 8))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.LazyConv2d(1, kernel_size=3)\n",
    "X = torch.rand(size=(8, 8))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 15])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size = (11, 3)\n",
    "X = torch.rand(size=(15, 15))\n",
    "pad_last_dim = [int(kernel_size[1] / 2),  int(kernel_size[1] / 2)]\n",
    "pad_first_dim = [int(kernel_size[0] / 2), int(kernel_size[0] / 2)]\n",
    "pad_val = (*pad_last_dim, *pad_first_dim)\n",
    "x_padded = F.pad( X, pad_val, 'constant', 0)\n",
    "\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(15, 10))\n",
    "# ax[0].imshow(x_padded)\n",
    "# ax[0].set_xticks([])\n",
    "# ax[0].set_yticks([])\n",
    "# ax[1].imshow(X)\n",
    "# ax[1].set_xticks([])\n",
    "# ax[1].set_yticks([])\n",
    "\n",
    "conv2d = nn.LazyConv2d(1, kernel_size=kernel_size)\n",
    "comp_conv2d(conv2d, x_padded).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride\n",
    "\n",
    "When computing the cross-correlation,\n",
    "we start with the convolution window\n",
    "at the upper-left corner of the input tensor,\n",
    "and then slide it over all locations both down and to the right.\n",
    "In the previous examples, we defaulted to sliding one element at a time.\n",
    "However, sometimes, either for computational efficiency\n",
    "or because we wish to downsample,\n",
    "we move our window more than one element at a time,\n",
    "skipping the intermediate locations. This is particularly useful if the convolution \n",
    "kernel is large since it captures a large area of the underlying image.\n",
    "\n",
    "We refer to the number of rows and columns traversed per slide as *stride*.\n",
    "So far, we have used strides of 1, both for height and width.\n",
    "Sometimes, we may want to use a larger stride.\n",
    ":numref:`img_conv_stride` shows a two-dimensional cross-correlation operation\n",
    "with a stride of 3 vertically and 2 horizontally.\n",
    "The shaded portions are the output elements as well as the input and kernel tensor elements used for the output computation: $0\\times0+0\\times1+1\\times2+2\\times3=8$, $0\\times0+6\\times1+0\\times2+0\\times3=6$.\n",
    "We can see that when the second element of the first column is generated,\n",
    "the convolution window slides down three rows.\n",
    "The convolution window slides two columns to the right\n",
    "when the second element of the first row is generated.\n",
    "When the convolution window continues to slide two columns to the right on the input,\n",
    "there is no output because the input element cannot fill the window\n",
    "(unless we add another column of padding).\n",
    "\n",
    "![Cross-correlation with strides of 3 and 2 for height and width, respectively.](../img/conv-stride.svg)\n",
    ":label:`img_conv_stride`\n",
    "\n",
    "In general, when the stride for the height is $s_\\textrm{h}$\n",
    "and the stride for the width is $s_\\textrm{w}$, the output shape is\n",
    "\n",
    "$$\\lfloor(n_\\textrm{h}-k_\\textrm{h}+p_\\textrm{h}+s_\\textrm{h})/s_\\textrm{h}\\rfloor \\times \\lfloor(n_\\textrm{w}-k_\\textrm{w}+p_\\textrm{w}+s_\\textrm{w})/s_\\textrm{w}\\rfloor.$$\n",
    "\n",
    "If we set $p_\\textrm{h}=k_\\textrm{h}-1$ and $p_\\textrm{w}=k_\\textrm{w}-1$,\n",
    "then the output shape can be simplified to\n",
    "$\\lfloor(n_\\textrm{h}+s_\\textrm{h}-1)/s_\\textrm{h}\\rfloor \\times \\lfloor(n_\\textrm{w}+s_\\textrm{w}-1)/s_\\textrm{w}\\rfloor$.\n",
    "Going a step further, if the input height and width\n",
    "are divisible by the strides on the height and width,\n",
    "then the output shape will be $(n_\\textrm{h}/s_\\textrm{h}) \\times (n_\\textrm{w}/s_\\textrm{w})$.\n",
    "\n",
    "Below, we [**set the strides on both the height and width to 2**],\n",
    "thus halving the input height and width.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1125,  0.0626],\n",
       "        [ 0.0193, -0.0944]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand([5, 5])\n",
    "conv2d = nn.LazyConv2d(1, kernel_size=(2, 2), stride=(3, 2))\n",
    "comp_conv2d(conv2d, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class Conv:\n",
    "\n",
    "    def __init__(self, kernel_size, bias=True, pad=[0, 0], stride=(0, 0)) -> None:\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.weights = torch.rand(kernel_size)\n",
    "        self.bias = torch.rand(1) if bias else 0\n",
    "\n",
    "        self.pad, self.stride = pad, stride\n",
    "\n",
    "    def __call__(self, x) -> Any:\n",
    "\n",
    "        x = F.pad(x, [self.pad[1], self.pad[1], self.pad[0], self.pad[0]])\n",
    "        h = (x.shape[0] + self.stride[0] - self.kernel_size[0]) / self.stride[0]\n",
    "        w = (x.shape[1] + self.stride[1] - self.kernel_size[1]) / self.stride[1]\n",
    "        h, w = int(h), int(w)\n",
    "        res = torch.zeros((h, w))\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                res[i, j] = (x[i * self.stride[0]: i * self.stride[0] + self.kernel_size[0], j * self.stride[1] :j * self.stride[1] + self.kernel_size[1] ] * self.weights).sum() + self.bias\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "def test_corr2d_final():\n",
    "\n",
    "    x = torch.rand((23, 23))\n",
    "\n",
    "    for ks in [(5, 3), (7, 3), (9, 7), (3, 11)]:\n",
    "        for pad in [(2, 1), (2, 2), (3, 5), (9, 13)]:\n",
    "            for st in [(1, 3), (3, 5), (7, 3)]:\n",
    "\n",
    "                    conv2d = nn.Conv2d(1, 1,  kernel_size=ks, stride=st, padding=pad)\n",
    "                    corr = Conv(kernel_size=ks, bias=True, pad=pad, stride=st)\n",
    "\n",
    "\n",
    "                    corr.weights = conv2d.weight.reshape(ks)\n",
    "                    corr.bias = conv2d.bias.reshape(-1)\n",
    "\n",
    "                    assert ((corr(x) - conv2d(x.reshape(1, 1, *x.shape))).abs() > 1e-6).sum() == 0, \"naive conv did not match torch result\"\n",
    "\n",
    "test_corr2d_final()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
